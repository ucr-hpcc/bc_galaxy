---
cluster: "hpcc"
attributes:
  modules: "miniconda3/ slurm/ slurm-drmaa/ workspace/"
  bc_account: null
  bc_num_slots: 1
  num_cores:
    widget: "number_field"
    label: "Number of cores"
    value: 4
    min: 1
    id: 'num_cores'
  num_mem:
    widget: "number_field"
    label: "Memory in GB"
    value: 8
    min: 1
    id: 'num_mem'
  auto_queues:
    help: |
      For more details on our Queue Policies, see our <a href="https://hpcc.ucr.edu/manuals/hpc_cluster/queue/" target="_blank">Queue Policies</a> page.
  working_dir:
    label: "Database Directory"
    data-filepicker: true
    data-target-file-type: dirs
    readonly: false
    help: "Select where your galaxy database will be or is currently located. Defaults to your $HOME directory"
  runtime:
    widget: "text_field"
    label: "Job runtime"
    pattern: "[0-9:-]*"
    value: "2-00:00:00"
    help: |
      In the format of DD-HH:MM:SS. Eg. 2 days, 13 hours, 37 minutes, 42 seconds would be "2-13:37:42".<br/>
      Max runtime is determined by the queue, see <a href="https://hpcc.ucr.edu/manuals/hpc_cluster/queue/" target="_blank">Queue Policies</a> for more details.
  extra_slurm:
    widget: "text_field"
    label: "Additional Slurm Arguments"
    value: ""
    help: "Additional arguments to pass to slurm. eg '--gres=gpu:1' will be needed if running on the GPU partition."
  job_runner:
    widget: select
    label: "Galaxy tool runner"
    help: |
      - **local** - When running tools locally, they are child processes of the Galaxy server. 
        The maximum number of concurrent jobs is the number of cores.
        When session ends, you lose contact with those jobs still running, 
        and they must be restarted.
      - **cluster** - (STILL IN DEVELOPMENT!)When running tools in a cluster, they are separate processes from the Galaxy server.
        They are jobs submitted to Slurm as configured above and there is no maximum number of concurrent jobs. 
        When session ends, the jobs will continue to run and finish.
    options:
      - [ "local",     "local"]
      - [ "cluster", "slurm" ]
form:
  - modules
  - working_dir
  - num_cores
  - num_mem
  - runtime
  - auto_queues
  - extra_slurm
  - job_runner
